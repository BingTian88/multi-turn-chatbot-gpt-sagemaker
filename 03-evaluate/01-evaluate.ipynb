{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0eda05a-3855-4953-bbae-b506f4fc05d4",
   "metadata": {},
   "source": [
    "##### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f476870-9115-4a8e-9047-2f57424963f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install torch==1.12.1+cu113\n",
    "!pip install transformers==4.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de50f2-d4ea-4d18-aaf3-16038534a491",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33bf3309-ea9f-46c2-b84b-ae36c7f1247b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.nn import functional as F\n",
    "from itertools import chain\n",
    "import transformers \n",
    "import logging\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af530362-4671-4a5b-959a-5eb676fe537f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd5442-166b-4e5c-bf07-3a14cba21b8d",
   "metadata": {},
   "source": [
    "##### Setup logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "646e35a4-0e2a-447b-ada2-a3a9fe78d78b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173e271-d19a-46c3-b57e-8ad52a24e954",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98a5c12-728d-4a46-86a8-6885ebb43163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using transformers version: 4.21.0]\n",
      "[Using torch version: 1.13.1+cu117]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using transformers version: {transformers.__version__}]')\n",
    "logger.info(f'[Using torch version: {torch.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9f9ba-e7ff-435d-b089-8141588c8f72",
   "metadata": {},
   "source": [
    "#### Setup essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4a469-aad6-451a-aff9-ebf59ede6e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e3918b5-1a87-4956-97f2-0ad88b620c65",
   "metadata": {},
   "source": [
    "#### Load GPT-Neo tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f37987-5311-40db-8995-183c00573e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-neo-125M', vocab_size=50257, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "logger.info(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947779cd-ba55-402f-9958-eacdd21f1ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    'bos_token': '<|startoftext|>',\n",
    "    'additional_special_tokens': ['<|speaker-1|>', '<|speaker-2|>', '<|pad|>', '<|mask|>']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7cd127-51b4-48a3-a1bb-af52aff5f27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = tokenizer.add_special_tokens(special_tokens)\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8740f8e2-4d5e-4085-b7aa-c5e2ce5e365f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-neo-125M', vocab_size=50257, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'additional_special_tokens': ['<|speaker-1|>', '<|speaker-2|>', '<|pad|>', '<|mask|>']})\n"
     ]
    }
   ],
   "source": [
    "logger.info(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3ba74-7c20-45a9-9ae9-75cea8bbebf2",
   "metadata": {},
   "source": [
    "#### Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97106876-4e08-4db4-b236-438b1654c45a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained('./../02-train/model')\n",
    "model.resize_token_embeddings(len(vocab))\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "logger.info(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72bb01-635c-4046-ab70-0e7bf01c227e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f527631b-c572-4fca-bf34-0781cbda6a30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "d989d1d4-32b9-49a0-b692-e119a70531d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4db1edd7-f766-4048-8111-f67195f1158e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10940720-3411-4977-8de7-08a07ece251a",
   "metadata": {},
   "source": [
    "#### Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd5b48b8-878e-4d40-9753-9356bd467f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bos_id = vocab['<|startoftext|>']\n",
    "eos_id = vocab['<|endoftext|>']\n",
    "speaker_1_id = vocab['<|speaker-1|>']\n",
    "speaker_2_id = vocab['<|speaker-2|>']\n",
    "mask = vocab['<|mask|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3a93a1-67d7-4501-9b5e-13f75b630989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(query: str) -> (torch.LongTensor, torch.LongTensor, int):\n",
    "    # Initialize empty list to store input ids for each turn\n",
    "    input_ids_turns = []\n",
    "    \n",
    "    # Add speaker 1 id to start of query and encode it using the tokenizer\n",
    "    input_ids = tokenizer.encode(query)\n",
    "    input_ids = [speaker_1_id] + input_ids\n",
    "    input_ids_turns.append(input_ids)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add beginning of sequence and end of sequence ids to input_ids, and convert it to a tensor\n",
    "    input_ids = [bos_id] + list(chain.from_iterable(input_ids_turns)) + [speaker_2_id]\n",
    "    \n",
    "    # Determine the speaker of the first turn based on the first speaker id\n",
    "    start_sp_id = input_ids_turns[0][0]\n",
    "    # Determine the speaker of the next turn\n",
    "    next_sp_id = speaker_1_id if start_sp_id == speaker_2_id else speaker_2_id\n",
    "    \n",
    "    # Create token type ids for each turn based on the speaker of the turn\n",
    "    token_type_ids = [[start_sp_id] * len(turn) if h % 2 == 0 else [next_sp_id] * len(turn) for h, turn in enumerate(input_ids_turns)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add beginning of sequence and end of sequence ids to token_type_ids, and convert it to a tensor\n",
    "    token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [speaker_2_id]\n",
    "    \n",
    "    # Determine the length of the input_ids tensor\n",
    "    input_len = len(input_ids)\n",
    "    \n",
    "    input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "    token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    return input_ids, token_type_ids, input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "974b624e-9213-46e3-8ac2-5585d9ea1835",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50257, 50258,    72,  5465,   616,  1204, 50259]], device='cuda:0'),\n",
       " tensor([[50258, 50258, 50258, 50258, 50258, 50258, 50259]], device='cuda:0'),\n",
       " 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('i hate my life')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86b947-4123-41b2-9bd3-88fd61b50b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b9390c0-cb7f-4c78-8835-543eb7943c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "top_p = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f70feffb-b36f-480f-aca4-f62cc262c828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(query: str) -> str:\n",
    "    output_ids = []\n",
    "    \n",
    "    input_ids, token_type_ids, input_len = tokenize(query)\n",
    "    \n",
    "    print(input_ids)\n",
    "    print(token_type_ids)\n",
    "    print(input_len)\n",
    "    \n",
    "    for i in range(input_len, MAX_LEN):\n",
    "        output = model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, i-1]\n",
    "        # Apply the softmax function to the logits\n",
    "        probs = F.softmax(output, dim=-1)\n",
    "\n",
    "        sorted_probs, sorted_idxs = torch.sort(probs, descending=True)\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)\n",
    " \n",
    "        idx_remove = cumsum_probs > top_p\n",
    "        idx_remove[:, 1:] = idx_remove[:, :-1].clone()\n",
    "        idx_remove[:, 0] = False\n",
    "        sorted_probs[idx_remove] = 0.0\n",
    "        sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)\n",
    "        probs = torch.zeros(output.shape, device=device).scatter_(-1, sorted_idxs, sorted_probs)\n",
    "        idx = torch.multinomial(probs, num_samples=1)\n",
    "        idx_item = idx.squeeze(-1).squeeze(-1).item()\n",
    "        output_ids.append(idx_item)\n",
    "        if idx_item == eos_id:\n",
    "            break\n",
    "\n",
    "        input_ids = torch.cat((input_ids, idx), dim=-1)\n",
    "        next_type_id = torch.LongTensor([[speaker_2_id]]).to(device)\n",
    "        token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    #response = tokenizer.decode(output_ids)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf0c4b55-a380-457f-857e-d80e25ec1bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50257, 50258, 33331,   502,   546,  3511,  5633, 50259]],\n",
      "       device='cuda:0')\n",
      "tensor([[50258, 50258, 50258, 50258, 50258, 50258, 50258, 50259]],\n",
      "       device='cuda:0')\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my bad today is just chilling for now'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('tell me about yourself ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec552e3-26d4-4fe2-b1d7-10acb959f356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8d312-919f-4ed0-97ba-9c33dfa2b703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05384ecc-815e-4719-bf7f-c3c9f03a0b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-gpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
