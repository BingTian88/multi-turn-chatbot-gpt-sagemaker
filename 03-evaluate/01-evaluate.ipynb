{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0eda05a-3855-4953-bbae-b506f4fc05d4",
   "metadata": {},
   "source": [
    "##### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f476870-9115-4a8e-9047-2f57424963f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install torch==1.12.1+cu113\n",
    "!pip install transformers==4.21.0\n",
    "!pip install ipywidgets==8.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de50f2-d4ea-4d18-aaf3-16038534a491",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bf3309-ea9f-46c2-b84b-ae36c7f1247b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.nn import functional as F\n",
    "from itertools import chain\n",
    "import transformers \n",
    "import logging\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af530362-4671-4a5b-959a-5eb676fe537f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd5442-166b-4e5c-bf07-3a14cba21b8d",
   "metadata": {},
   "source": [
    "##### Setup logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646e35a4-0e2a-447b-ada2-a3a9fe78d78b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173e271-d19a-46c3-b57e-8ad52a24e954",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98a5c12-728d-4a46-86a8-6885ebb43163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using transformers version: 4.21.0]\n",
      "[Using torch version: 1.12.1+cu113]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using transformers version: {transformers.__version__}]')\n",
    "logger.info(f'[Using torch version: {torch.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3918b5-1a87-4956-97f2-0ad88b620c65",
   "metadata": {},
   "source": [
    "#### Load GPT-Neo tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f37987-5311-40db-8995-183c00573e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-neo-125M', vocab_size=50257, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "logger.info(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947779cd-ba55-402f-9958-eacdd21f1ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    'bos_token': '<|startoftext|>',\n",
    "    'additional_special_tokens': ['<|speaker-1|>', '<|speaker-2|>', '<|pad|>', '<|mask|>']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7cd127-51b4-48a3-a1bb-af52aff5f27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = tokenizer.add_special_tokens(special_tokens)\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8740f8e2-4d5e-4085-b7aa-c5e2ce5e365f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-neo-125M', vocab_size=50257, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'additional_special_tokens': ['<|speaker-1|>', '<|speaker-2|>', '<|pad|>', '<|mask|>']})\n"
     ]
    }
   ],
   "source": [
    "logger.info(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3ba74-7c20-45a9-9ae9-75cea8bbebf2",
   "metadata": {},
   "source": [
    "#### Load fine-tuned GPT-Neo model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97106876-4e08-4db4-b236-438b1654c45a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained('./../02-train/model')\n",
    "model.resize_token_embeddings(len(vocab))\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "logger.info(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db1edd7-f766-4048-8111-f67195f1158e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10940720-3411-4977-8de7-08a07ece251a",
   "metadata": {},
   "source": [
    "#### Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5b48b8-878e-4d40-9753-9356bd467f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bos_id = vocab['<|startoftext|>']\n",
    "eos_id = vocab['<|endoftext|>']\n",
    "speaker_1_id = vocab['<|speaker-1|>']\n",
    "speaker_2_id = vocab['<|speaker-2|>']\n",
    "mask = vocab['<|mask|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a67f0e0-d459-4da4-b224-535e2a60066b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bos_id = 50257\n",
      "eos_id = 50256\n",
      "speaker_1_id = 50258\n",
      "speaker_2_id = 50259\n",
      "mask = 50261\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'bos_id = {bos_id}')\n",
    "logger.info(f'eos_id = {eos_id}')\n",
    "logger.info(f'speaker_1_id = {speaker_1_id}')\n",
    "logger.info(f'speaker_2_id = {speaker_2_id}')\n",
    "logger.info(f'mask = {mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b9390c0-cb7f-4c78-8835-543eb7943c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TOP_P = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29f4e6b0-8e58-4bbb-b9bd-241c2d48cdbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nucleus_sampling(input_ids, token_type_ids, input_len) -> str:\n",
    "    output_ids = []\n",
    "    \n",
    "    # iterate over the maximum possible length of the generated sequence\n",
    "    for i in range(input_len, MAX_LEN):\n",
    "        # get the model's output for the next token\n",
    "        output = model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, i-1]\n",
    "        \n",
    "        # apply the softmax function to convert the output logits into probabilities\n",
    "        probs = F.softmax(output, dim=-1)\n",
    "\n",
    "        # sort the probabilities in descending order\n",
    "        sorted_probs, sorted_idxs = torch.sort(probs, descending=True)\n",
    "        \n",
    "        # compute the cumulative sum of probabilities\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)\n",
    " \n",
    "        # identify the indices to remove (i.e., those with cumsum greater than the given probability threshold)\n",
    "        idx_remove = cumsum_probs > TOP_P\n",
    "        idx_remove[:, 1:] = idx_remove[:, :-1].clone()\n",
    "        idx_remove[:, 0] = False\n",
    "        \n",
    "        # zero out the probabilities of the indices to remove, then renormalize the probabilities\n",
    "        sorted_probs[idx_remove] = 0.0\n",
    "        sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)\n",
    "        probs = torch.zeros(output.shape, device=device).scatter_(-1, sorted_idxs, sorted_probs)\n",
    "        \n",
    "        # sample the next token from the probability distribution\n",
    "        idx = torch.multinomial(probs, num_samples=1)\n",
    "        idx_item = idx.squeeze(-1).squeeze(-1).item()\n",
    "        \n",
    "        # add the sampled token to the list of output ids\n",
    "        output_ids.append(idx_item)\n",
    "        \n",
    "        # stop generation if the end-of-sequence token is generated\n",
    "        if idx_item == eos_id:\n",
    "            break\n",
    "\n",
    "        # update the input ids and token type ids for the next iteration\n",
    "        input_ids = torch.cat((input_ids, idx), dim=-1)\n",
    "        next_type_id = torch.LongTensor([[speaker_2_id]]).to(device)\n",
    "        token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)\n",
    "        \n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3e406-9169-4280-8173-58f63e9c42f4",
   "metadata": {},
   "source": [
    "### Interactive Chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51a3b11a-4dfc-471e-870e-88abe53d6f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RESET_PROMPT = 'reset'\n",
    "MAX_TURNS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06a8d312-919f-4ed0-97ba-9c33dfa2b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    logger.info('[Entering chat session ...]')\n",
    "    logger.info(f'To quit the conversation and reset memory, please type \"{RESET_PROMPT}\"')\n",
    "    \n",
    "    query_history = []\n",
    "            \n",
    "    while True:\n",
    "        utterance = input('You: ')\n",
    "        \n",
    "        # Exit session if user types the RESET prompt\n",
    "        if utterance == RESET_PROMPT:\n",
    "            logger.info(f'[Exiting chat session]')\n",
    "            break\n",
    "            \n",
    "        # Add speaker 1 id to start of query and encode it using the tokenizer\n",
    "        input_ids = tokenizer.encode(utterance)\n",
    "        input_ids = [speaker_1_id] + input_ids\n",
    "        query_history.append(input_ids)\n",
    "        \n",
    "        if len(query_history) >= MAX_TURNS:\n",
    "            num_exceeded = len(query_history) - MAX_TURNS\n",
    "            query_history = query_history[num_exceeded:]\n",
    "            \n",
    "        # Add beginning of sequence and end of sequence ids to input_ids, and convert it to a tensor\n",
    "        input_ids = [bos_id] + list(chain.from_iterable(query_history)) + [speaker_2_id]\n",
    "\n",
    "        # Determine the speaker of the first turn based on the first speaker id\n",
    "        start_sp_id = query_history[0][0]\n",
    "        \n",
    "        # Determine the speaker of the next turn\n",
    "        next_sp_id = speaker_1_id if start_sp_id == speaker_2_id else speaker_2_id\n",
    "\n",
    "        # Create token type ids for each turn based on the speaker of the turn\n",
    "        token_type_ids = [[start_sp_id] * len(turn) if h % 2 == 0 else [next_sp_id] * len(turn) for h, turn in enumerate(query_history)]\n",
    "\n",
    "        # Add beginning of sequence and end of sequence ids to token_type_ids, and convert it to a tensor\n",
    "        token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [speaker_2_id]\n",
    "\n",
    "        # Determine the length of the input_ids tensor\n",
    "        input_len = len(input_ids)\n",
    "        \n",
    "        # Convert input_ids and token_type_ids to PyTorch tensors, add an extra dimension, and move to the device (GPU)\n",
    "        input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "        token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(device)\n",
    "        \n",
    "        # output_ids = nucleus_sampling(input_ids, token_type_ids, input_len)   \n",
    "        \n",
    "        # generate a response from the model given some input\n",
    "        output_ids = model.generate(input_ids=input_ids, \n",
    "                                    token_type_ids=token_type_ids, \n",
    "                                    pad_token_id=eos_id, \n",
    "                                    do_sample=True, \n",
    "                                    top_p=TOP_P, \n",
    "                                    max_length=MAX_LEN)\n",
    "        \n",
    "        # extract the generated sequence from the output and remove the input sequence\n",
    "        output_ids = output_ids[0].tolist()[input_len:]\n",
    "        \n",
    "        # convert the generated sequence of token ids into text\n",
    "        response = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        print(f'Bot: {response}')\n",
    "        \n",
    "        # append the generated sequence to the query history as token ids\n",
    "        query_history.append([speaker_2_id] + tokenizer.encode(response))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "14fa27e7-2be4-453b-b7f7-d25419fb8dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Entering chat session ...]\n",
      "To quit the conversation and reset memory, please type \"reset\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hola\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: hello how are you doing today\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  nothing much just chilling at home\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: huh?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: that sounds like you\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  lol\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i think i might have a few myself\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  few of what ?!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: and a lot of them were the last of the four little ones\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  i don't understand\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i am a lot of yourself\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Exiting chat session]\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1263ff-408d-4ab4-9d4e-7dd2affbbd23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g4dn.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
