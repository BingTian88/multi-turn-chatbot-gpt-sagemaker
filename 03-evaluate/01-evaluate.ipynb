{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0eda05a-3855-4953-bbae-b506f4fc05d4",
   "metadata": {},
   "source": [
    "##### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f476870-9115-4a8e-9047-2f57424963f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install torch==1.12.1+cu113\n",
    "!pip install transformers==4.21.0\n",
    "!pip install ipywidgets==8.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de50f2-d4ea-4d18-aaf3-16038534a491",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bf3309-ea9f-46c2-b84b-ae36c7f1247b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.nn import functional as F\n",
    "from getpass import getpass\n",
    "from itertools import chain\n",
    "import transformers \n",
    "import logging\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af530362-4671-4a5b-959a-5eb676fe537f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd5442-166b-4e5c-bf07-3a14cba21b8d",
   "metadata": {},
   "source": [
    "##### Setup logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646e35a4-0e2a-447b-ada2-a3a9fe78d78b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173e271-d19a-46c3-b57e-8ad52a24e954",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98a5c12-728d-4a46-86a8-6885ebb43163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using transformers version: 4.21.0]\n",
      "[Using torch version: 1.12.1+cu113]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using transformers version: {transformers.__version__}]')\n",
    "logger.info(f'[Using torch version: {torch.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9f9ba-e7ff-435d-b089-8141588c8f72",
   "metadata": {},
   "source": [
    "#### Setup essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4a469-aad6-451a-aff9-ebf59ede6e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e3918b5-1a87-4956-97f2-0ad88b620c65",
   "metadata": {},
   "source": [
    "#### Load GPT-Neo tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f37987-5311-40db-8995-183c00573e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-neo-125M', vocab_size=50257, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
    "logger.info(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947779cd-ba55-402f-9958-eacdd21f1ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    'bos_token': '<|startoftext|>',\n",
    "    'additional_special_tokens': ['<|speaker-1|>', '<|speaker-2|>', '<|pad|>', '<|mask|>']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7cd127-51b4-48a3-a1bb-af52aff5f27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = tokenizer.add_special_tokens(special_tokens)\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8740f8e2-4d5e-4085-b7aa-c5e2ce5e365f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='EleutherAI/gpt-neo-125M', vocab_size=50257, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'additional_special_tokens': ['<|speaker-1|>', '<|speaker-2|>', '<|pad|>', '<|mask|>']})\n"
     ]
    }
   ],
   "source": [
    "logger.info(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3ba74-7c20-45a9-9ae9-75cea8bbebf2",
   "metadata": {},
   "source": [
    "#### Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97106876-4e08-4db4-b236-438b1654c45a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained('./../02-train/model')\n",
    "model.resize_token_embeddings(len(vocab))\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "logger.info(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db1edd7-f766-4048-8111-f67195f1158e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10940720-3411-4977-8de7-08a07ece251a",
   "metadata": {},
   "source": [
    "#### Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5b48b8-878e-4d40-9753-9356bd467f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bos_id = vocab['<|startoftext|>']\n",
    "eos_id = vocab['<|endoftext|>']\n",
    "speaker_1_id = vocab['<|speaker-1|>']\n",
    "speaker_2_id = vocab['<|speaker-2|>']\n",
    "mask = vocab['<|mask|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a67f0e0-d459-4da4-b224-535e2a60066b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bos_id = 50257\n",
      "eos_id = 50256\n",
      "speaker_1_id = 50258\n",
      "speaker_2_id = 50259\n",
      "mask = 50261\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'bos_id = {bos_id}')\n",
    "logger.info(f'eos_id = {eos_id}')\n",
    "logger.info(f'speaker_1_id = {speaker_1_id}')\n",
    "logger.info(f'speaker_2_id = {speaker_2_id}')\n",
    "logger.info(f'mask = {mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3a93a1-67d7-4501-9b5e-13f75b630989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(query: str) -> (torch.LongTensor, torch.LongTensor, int):\n",
    "    # Initialize empty list to store input ids for each turn\n",
    "    input_ids_turns = []\n",
    "    \n",
    "    # Add speaker 1 id to start of query and encode it using the tokenizer\n",
    "    input_ids = tokenizer.encode(query)\n",
    "    input_ids = [speaker_1_id] + input_ids\n",
    "    input_ids_turns.append(input_ids)\n",
    "    \n",
    "    # Add beginning of sequence and end of sequence ids to input_ids, and convert it to a tensor\n",
    "    input_ids = [bos_id] + list(chain.from_iterable(input_ids_turns)) + [speaker_2_id]\n",
    "    \n",
    "    # Determine the speaker of the first turn based on the first speaker id\n",
    "    start_sp_id = input_ids_turns[0][0]\n",
    "    # Determine the speaker of the next turn\n",
    "    next_sp_id = speaker_1_id if start_sp_id == speaker_2_id else speaker_2_id\n",
    "    \n",
    "    # Create token type ids for each turn based on the speaker of the turn\n",
    "    token_type_ids = [[start_sp_id] * len(turn) if h % 2 == 0 else [next_sp_id] * len(turn) for h, turn in enumerate(input_ids_turns)]\n",
    "    \n",
    "    # Add beginning of sequence and end of sequence ids to token_type_ids, and convert it to a tensor\n",
    "    token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [speaker_2_id]\n",
    "    \n",
    "    # Determine the length of the input_ids tensor\n",
    "    input_len = len(input_ids)\n",
    "    \n",
    "    input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "    token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    return input_ids, token_type_ids, input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b9390c0-cb7f-4c78-8835-543eb7943c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TOP_P = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f70feffb-b36f-480f-aca4-f62cc262c828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nucleus_sampling(input_ids, token_type_ids, input_len) -> str:\n",
    "    output_ids = []\n",
    "    \n",
    "    for i in range(input_len, MAX_LEN):\n",
    "        output = model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, i-1]\n",
    "        # Apply the softmax function to the logits\n",
    "        probs = F.softmax(output, dim=-1)\n",
    "\n",
    "        sorted_probs, sorted_idxs = torch.sort(probs, descending=True)\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)\n",
    " \n",
    "        idx_remove = cumsum_probs > TOP_P\n",
    "        idx_remove[:, 1:] = idx_remove[:, :-1].clone()\n",
    "        idx_remove[:, 0] = False\n",
    "        sorted_probs[idx_remove] = 0.0\n",
    "        sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)\n",
    "        probs = torch.zeros(output.shape, device=device).scatter_(-1, sorted_idxs, sorted_probs)\n",
    "        idx = torch.multinomial(probs, num_samples=1)\n",
    "        idx_item = idx.squeeze(-1).squeeze(-1).item()\n",
    "        output_ids.append(idx_item)\n",
    "        \n",
    "        if idx_item == eos_id:\n",
    "            break\n",
    "\n",
    "        input_ids = torch.cat((input_ids, idx), dim=-1)\n",
    "        next_type_id = torch.LongTensor([[speaker_2_id]]).to(device)\n",
    "        token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3e406-9169-4280-8173-58f63e9c42f4",
   "metadata": {},
   "source": [
    "#### Interactive Chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51a3b11a-4dfc-471e-870e-88abe53d6f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RESET_CMD = 'reset'\n",
    "MAX_TURNS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06a8d312-919f-4ed0-97ba-9c33dfa2b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    logger.info('[Entering chat session ...]')\n",
    "    logger.info(f'To quit the conversation and reset memory, please type \"{RESET_CMD}\"')\n",
    "    query_history = []\n",
    "            \n",
    "    while True:\n",
    "        utterance = input('You: ')\n",
    "        if utterance == RESET_CMD:\n",
    "            logger.info(f'[Exiting chat session]')\n",
    "            break\n",
    "            \n",
    "        # Add speaker 1 id to start of query and encode it using the tokenizer\n",
    "        input_ids = tokenizer.encode(utterance)\n",
    "        input_ids = [speaker_1_id] + input_ids\n",
    "        query_history.append(input_ids)\n",
    "        \n",
    "        if len(query_history) >= MAX_TURNS:\n",
    "            num_exceeded = len(query_history) - MAX_TURNS + 1\n",
    "            query_history = query_history[num_exceeded:]\n",
    "            \n",
    "        # Add beginning of sequence and end of sequence ids to input_ids, and convert it to a tensor\n",
    "        input_ids = [bos_id] + list(chain.from_iterable(query_history)) + [speaker_2_id]\n",
    "\n",
    "        # Determine the speaker of the first turn based on the first speaker id\n",
    "        start_sp_id = query_history[0][0]\n",
    "        # Determine the speaker of the next turn\n",
    "        next_sp_id = speaker_1_id if start_sp_id == speaker_2_id else speaker_2_id\n",
    "\n",
    "        # Create token type ids for each turn based on the speaker of the turn\n",
    "        token_type_ids = [[start_sp_id] * len(turn) if h % 2 == 0 else [next_sp_id] * len(turn) for h, turn in enumerate(query_history)]\n",
    "\n",
    "        # Add beginning of sequence and end of sequence ids to token_type_ids, and convert it to a tensor\n",
    "        token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [speaker_2_id]\n",
    "\n",
    "        # Determine the length of the input_ids tensor\n",
    "        input_len = len(input_ids)\n",
    "\n",
    "        input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "        token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(device)\n",
    "        \n",
    "        #output_ids = nucleus_sampling(input_ids, token_type_ids, input_len)   \n",
    "        \n",
    "        \n",
    "        output_ids = model.generate(input_ids=input_ids, \n",
    "                                    token_type_ids=token_type_ids, \n",
    "                                    pad_token_id=eos_id, \n",
    "                                    do_sample=True, \n",
    "                                    top_p=TOP_P, \n",
    "                                    max_length=MAX_LEN, \n",
    "                                    output_hidden_states=True, \n",
    "                                    output_scores=True, \n",
    "                                    return_dict_in_generate=True).sequences\n",
    "\n",
    "        output_ids = output_ids[0].tolist()[input_len:]        \n",
    "        response = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        print(f'Bot: {response}')\n",
    "        query_history.append([speaker_2_id] + tokenizer.encode(response))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6a69896-4604-4435-a368-b4ab8ac0ee35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bos_id = 50257\n",
      "eos_id = 50256\n",
      "speaker_1_id = 50258\n",
      "speaker_2_id = 50259\n",
      "mask = 50261\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'bos_id = {bos_id}')\n",
    "logger.info(f'eos_id = {eos_id}')\n",
    "logger.info(f'speaker_1_id = {speaker_1_id}')\n",
    "logger.info(f'speaker_2_id = {speaker_2_id}')\n",
    "logger.info(f'mask = {mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14fa27e7-2be4-453b-b7f7-d25419fb8dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Entering chat session ...]\n",
      "To quit the conversation and reset memory, please type \"reset\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50257, 50258,  5303, 50259, 31373,   764,   703,   389,   345,  5633,\n",
      "         50256]], device='cuda:0')\n",
      "<class 'torch.Tensor'>\n",
      "[31373, 764, 703, 389, 345, 5633, 50256]\n",
      "Bot: hello. how are you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  i am very good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50257, 50259, 31373,    13,   703,   389,   345,    30, 50258,    72,\n",
      "           716,   845,   922, 50259,    72,   588,  1016,   284,   262, 11550,\n",
      "         10908,   284,  1394,  4197,   764, 50256]], device='cuda:0')\n",
      "<class 'torch.Tensor'>\n",
      "[72, 588, 1016, 284, 262, 11550, 10908, 284, 1394, 4197, 764, 50256]\n",
      "Bot: i like going to the gym everyday to keep fit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  i should do it too!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50257, 50259,    72,   588,  1016,   284,   262, 11550, 10908,   284,\n",
      "          1394,  4197,    13, 50258,    72,   815,   466,   340,  1165,     0,\n",
      "         50259, 43911,   706,   281,  8855,   287,  8591,   764, 50256]],\n",
      "       device='cuda:0')\n",
      "<class 'torch.Tensor'>\n",
      "[43911, 706, 281, 8855, 287, 8591, 764, 50256]\n",
      "Bot: morning after an adventure in la.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  reset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Exiting chat session]\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f014a-749a-4338-8882-99f9a9cbce4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a707fe9c-99d2-4aac-ac87-c2e726b4715c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e05d27cc-0ec5-4d17-b5c5-648d911de17b",
   "metadata": {},
   "source": [
    "### Evaluate directly from the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab491edd-c358-4de3-a97a-2be774def809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba3898-c2c7-4040-981f-60b071265187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cd739-7708-40a3-965c-18205a79ce01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1263ff-408d-4ab4-9d4e-7dd2affbbd23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g4dn.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
